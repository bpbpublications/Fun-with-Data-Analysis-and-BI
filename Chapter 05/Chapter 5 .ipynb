{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# This code is used to suppress warnings in the console output.\n",
    "# The `warnings` module is imported, and the `filterwarnings()` function is called with the argument \"ignore\".\n",
    "# This sets the warning filter to ignore all warnings, preventing them from being displayed in the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excited try new product \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", tweet)\n",
    "    \n",
    "    # Remove hashtags and mentions\n",
    "    tweet = re.sub(r\"#\\w+|\\@\\w+\", \"\", tweet)\n",
    "    \n",
    "    # Tokenize tweet\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Remove punctuation and special characters\n",
    "    filtered_tweet = \" \".join(filtered_tokens)\n",
    "    filtered_tweet = re.sub(r\"[^\\w\\s]\", \"\", filtered_tweet)\n",
    "    \n",
    "    return filtered_tweet\n",
    "\n",
    "# Example usage\n",
    "tweet = \"Excited to try out the new product! #awesome #innovation\"\n",
    "preprocessed_tweet = preprocess_tweet(tweet)\n",
    "print(preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon-Based approaches for twitter sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon', quiet = True)\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(tweet):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sid.polarity_scores(tweet)\n",
    "    \n",
    "    # Extract the compound score\n",
    "    sentiment_score = sentiment_scores[\"compound\"]\n",
    "    \n",
    "    if sentiment_score >= 0.05:\n",
    "        sentiment = \"positive\"\n",
    "    elif sentiment_score <= -0.05:\n",
    "        sentiment = \"negative\"\n",
    "    else:\n",
    "        sentiment = \"neutral\"\n",
    "    \n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "tweet = \"I love this new product!\"\n",
    "sentiment = analyze_sentiment(tweet)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Twitter Authentification Credentials\n",
    "Please update with your own credentials\n",
    "\"\"\"\n",
    "cons_key = ''\n",
    "cons_secret = ''\n",
    "acc_token = ''\n",
    "acc_secret = ''\n",
    "\n",
    "# (1). Authentication Function\n",
    "def get_twitter_auth():\n",
    "    \"\"\"\n",
    "    @return:\n",
    "        - the authentication to Twitter\n",
    "    \"\"\"\n",
    "    try:\n",
    "        consumer_key = cons_key\n",
    "        consumer_secret = cons_secret\n",
    "        access_token = acc_token\n",
    "        access_secret = acc_secret\n",
    "        \n",
    "    except KeyError:\n",
    "        sys.stderr.write(\"Twitter Environment Variable not Set\\n\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "    \n",
    "    return auth\n",
    "\n",
    "# (2). Client function to access the authentication API\n",
    "def get_twitter_client():\n",
    "    \"\"\"\n",
    "    @return:\n",
    "        - the client to access the authentication API\n",
    "    \"\"\"\n",
    "    auth = get_twitter_auth()\n",
    "    client = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    return client\n",
    "\n",
    "# (3). Function creating final dataframe\n",
    "def get_tweets_from_user(twitter_user_name, page_limit=16, count_tweet=200):\n",
    "    \"\"\"\n",
    "    @params:\n",
    "        - twitter_user_name: the twitter username of a user (company, etc.)\n",
    "        - page_limit: the total number of pages (max=16)\n",
    "        - count_tweet: maximum number to be retrieved from a page\n",
    "        \n",
    "    @return\n",
    "        - all the tweets from the user twitter_user_name\n",
    "    \"\"\"\n",
    "    client = get_twitter_client()\n",
    "    \n",
    "    all_tweets = []\n",
    "    \n",
    "    for page in Cursor(client.user_timeline, \n",
    "                        screen_name=twitter_user_name, \n",
    "                        count=count_tweet).pages(page_limit):\n",
    "        for tweet in page:\n",
    "            parsed_tweet = {}\n",
    "            parsed_tweet['date'] = tweet.created_at\n",
    "            parsed_tweet['author'] = tweet.user.name\n",
    "            parsed_tweet['twitter_name'] = tweet.user.screen_name\n",
    "            parsed_tweet['text'] = tweet.text\n",
    "            parsed_tweet['number_of_likes'] = tweet.favorite_count\n",
    "            parsed_tweet['number_of_retweets'] = tweet.retweet_count\n",
    "                \n",
    "            all_tweets.append(parsed_tweet)\n",
    "    \n",
    "    # Create dataframe \n",
    "    df = pd.DataFrame(all_tweets)\n",
    "    \n",
    "    # Remove duplicates if there are any\n",
    "    df = df.drop_duplicates(\"text\", keep='first')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for twitter sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('twitter_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                       yo look lit cs go overwatch combo\n",
      "1       attention executive administrator ever store g...\n",
      "2       f loving new dlc rhandlerr rhandlerr rhandlerr...\n",
      "3                                       rainbow6game xbox\n",
      "4                                  miss battlefield 1 day\n",
      "                              ...                        \n",
      "8239    thanks entering grand summoners watch video se...\n",
      "8240                              agree clearer excellent\n",
      "8241    worst minute ever locked layup missed open jum...\n",
      "8242    participating beta weekend absolute best snipi...\n",
      "8243                first week perfection mtg ignore tags\n",
      "Name: Processed_tweet, Length: 8244, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Preprocess the tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    # Remove Twitter handles\n",
    "    tweet = re.sub(r'@[A-Za-z0-9_]+', '', tweet)\n",
    "    \n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)\n",
    "    \n",
    "    # Remove special characters and punctuation\n",
    "    tweet = re.sub(r'\\W+', ' ', tweet)\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    # Join the tokens back into a single string\n",
    "    processed_tweet = ' '.join(tokens)\n",
    "    \n",
    "    return processed_tweet\n",
    "\n",
    "# Convert 'Tweet' column to string\n",
    "df['Tweet'] = df['Tweet'].astype(str)\n",
    "\n",
    "# Apply preprocessing to each tweet in the dataset\n",
    "df['Processed_tweet'] = df['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Print the preprocessed dataset\n",
    "print(df['Processed_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Processed_tweet']\n",
    "y = df['Sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert sentiment labels to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "y = y.map(label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6595,), (1649,), (6595,), (1649,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the LinearSVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the confusion matrix and generate classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Martrix: \n",
      "[[498  60  83]\n",
      " [102 275  92]\n",
      " [ 87  75 377]]\n",
      "\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.72      0.78      0.75       641\n",
      "     Neutral       0.67      0.59      0.63       469\n",
      "    Positive       0.68      0.70      0.69       539\n",
      "\n",
      "    accuracy                           0.70      1649\n",
      "   macro avg       0.69      0.69      0.69      1649\n",
      "weighted avg       0.70      0.70      0.70      1649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Martrix: ')\n",
    "print(cm)\n",
    "\n",
    "# Generate classification report \n",
    "target_names = ['Negative', 'Neutral', 'Positive']\n",
    "classification_rep = classification_report(y_test, y_pred, target_names = target_names)\n",
    "print('\\nClassification Report: ')\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
